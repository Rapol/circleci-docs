---
contentTags:
  platform:
    - Cloud
    - Server v4.x
---
= Large Language Model evaluation
:description: Docs page on setting up LLM evaluation in your CI/CD pipeline
:page-layout: classic-docs
:page-liquid:
:icons: font
:experimental:

This page describes common methods for running Large Language Model (LLM) evaluations on CircleCI.

== Evaluations overview

Evaluations, also known as evals, are a methodology for assessing the quality of AI software.

Evaluations provide insights into the performance and efficiency of applications based on LLMs, and allow teams to quantify how well their AI implementation works, measure improvements, and catch regressions.

The term _evaluation_ initially referred to a way to rate and compare AI models, but it has expanded to include application-level testing, including Retrieval Augmented Generation (RAG), function calling, and agent-based applications.

The evaluation process involves using a dataset of inputs to an LLM or application code, and a method to determine if the returned response matches the expected response.  There are numerous evaluation methodologies, including:

* LLM-assisted evaluations
* Human evaluations
* Intrinsic and extrinsic evaluations
* Bias and fairness checks
* Readability evaluations

Evaluations can cover many aspects of a model performance, including:

* Ability to comprehend specific jargon
* Make accurate predictions
* Avoid hallucinations and generate relevant content
* Respond in a fair and unbiased way, and within a specific style
* Avoid certain expressions

== Automating evaluations with CircleCI

Evaluations can be expressed as classic software tests, typically characterised by the "input, expected output, assertion" format, and as such they can be automated into CircleCI pipelines.

It is crucial to keep two important differences between evals and classic software tests in mind:

* LLMs are predominantly non-deterministic, leading to flaky evaluations, unlike deterministic software tests.
* Evaluation results are subjective. Small regressions in a metric might not necessarily be a cause for concern, unlike failing tests in regular software testing.

With CircleCI, you can define, automate, and run evaluations using your preferred evaluation framework. Through declaring the necessary commands in your `config.yml`, you can ensure these evaluations are run within your CircleCI pipeline.

Using an open source library or third-party tools can simplify defining evaluations, tracking progress, and reviewing the evaluation outcomes.

== The CircleCI AI evals orb

[NOTE]
====
The link:https://circleci.com/developer/orbs/orb/circleci/ai-evals[official AI evals orb] supports integrations with Braintrust and LangSmith.

If you run evaluations using a different tool, let us know at mailto:ai-feedback@circleci.com[]. You can also contribute directly to the official orb by opening a PR on the link:https://github.com/CircleCI-Public/ai-evals-orb[public repository].
====

CircleCI provides an link:https://circleci.com/developer/orbs/orb/circleci/ai-evals[official evaluations orb] that simplifies the definition and execution of evaluation jobs using popular third-party tools, and generates reports of evaluation results.

Given the volatile nature of evaluations, evaluations orchestrated by the CircleCI AI evals orb do not halt the pipeline if an evaluation fails. This approach ensures that the inherent flakiness of evaluations does not disrupt the development cycle.

Instead, a summary of the evaluation results is created and presented:

* As a comment on the corresponding GitHub pull request:
+
image::/docs/assets/img/docs/llmops/github-pr-comment.png[Jobs overview]

* As an artifact within the CircleCI User Interface:
+
image::/docs/assets/img/docs/llmops/artifact.png[Jobs overview]

You can review the summary, and, if required, proceed to a detailed analyses of the individual evaluation on your third party evaluation provider.

////
== Getting started with the evaluations orb

Link to tutorial which will live in another page ( under Tests > Tutorials)
Need code example in CircleCI-public

You can find a full tutorial of how to use the LLM-evaluations orb on this page.
////

== Storing credentials for your evaluations
CircleCI makes it easy to store your credentials for LLM providers as well as LLMOps tools. Navigate to menu:Project Settings[LLMOps] to enter, verify, and access your OpenAI secrets. You will also find a starting template for your `config.yml` file.

You can also save the credentials to your preferred evaluation platform, including Braintrust and LangSmith. These credentials can then be used when setting up a pipeline that leverages the AI evals orb.

To get started, navigate to menu:Project Settings[LLMOps]:

image::/docs/assets/img/docs/llmops/create-context.png[Jobs overview]

image::/docs/assets/img/docs/llmops/openai-context.png[Jobs overview]


